{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d918c55d",
   "metadata": {},
   "source": [
    "### Cinco diferenças entre o Random Forest e o AdaBoost:\n",
    "\n",
    "#### Florestas\n",
    "\n",
    "* Random Forest: são geradas várias árvores de modo aleatório, e essas árvores não tem profundidade ou quantidade de folhas limitadas. \n",
    "* AdaBoost:floresta de strumps (um nó e duas folhas) e cada strump influenciará na criação do strump posterior (bootstrap: amostragem com reposição). \n",
    "\n",
    "#### Treinamento de Modelos:\n",
    "\n",
    "* Random Forest: As árvores de decisão são treinadas de forma independente umas das outras, e cada árvore é ajustada com uma amostra aleatória dos dados e das características.\n",
    "* AdaBoost: Treina modelos sequencialmente, ajustando o foco para exemplos que foram mal classificados pelos modelos anteriores. O treinamento é adaptativo e os modelos são ajustados com base nos erros dos modelos anteriores.\n",
    "\n",
    "#### Peso das Respostas das Árvores:\n",
    "\n",
    "* Random Forest: Todas as árvores têm o mesmo peso e o modelo final é uma média das árvores treinadas.\n",
    "* AdaBoost: O modelo final é uma média ponderada das árvores, que leva em consideração o potencial preditivo de cada árvore.\n",
    "\n",
    "#### Operação de Ensembling:\n",
    "* Random Forest: Random Forest emprega montagem paralela. Forest processa árvores em paralelo, permitindo que jobs sejam paralelizados em uma máquina multiprocessadora.\n",
    "* Adaboost: faz uso de ensembling sequencial. Ele usa um método passo a passo.\n",
    "\n",
    "#### Cálculo de estimativa\t\n",
    "* Random Forest visa diminuir a variância (erro de um modelo em relação aos dados de teste), não o viés (quão bem um modelo se adequa aos dados de treino).\t\n",
    "* Adaboost visa diminuir o viés, não a variância."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3070d",
   "metadata": {},
   "source": [
    "### Cinco Hiperparâmetros importantes no AdaBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bfbfae",
   "metadata": {},
   "source": [
    "* base_estimator: Classificador ou Regressão, a depender da variável resposta. \n",
    "* n_estimators: controla o número de aprendizes fracos. \n",
    "* learning_rate:controla a contribuição dos aprendizes fracos na combinação final. Por padrão, aprendizes fracos são tocos de decisão.\n",
    "* algorithm: 'SAMME' é o algoritmo original do AdaBoost, enquanto 'SAMME.R' é uma versão mais recente e geralmente mais eficiente.\n",
    "* random_state: Define o valor para o gerador de números aleatórios usado no treinamento e na inicialização de pesos, serve para que futuramente você possa ter a mesma reprodutibilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748aec8",
   "metadata": {},
   "source": [
    "### Exemplo AdaBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a92ad333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c9656f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\",)\n",
    "scores = cross_val_score(clf, X, y, cv=5) #avalia o classificador usando 5 diferentes divisões dos dados e retorna a acurácia para cada divisão.\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd30976",
   "metadata": {},
   "source": [
    "### Exemplo AdaBoost Classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09e11f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
    "clf.fit(X, y)\n",
    "clf.predict([[0, 0, 0, 0]]) #Usa o classificador treinado para prever a classe de uma nova amostra com essas características\n",
    "\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663ddb1",
   "metadata": {},
   "source": [
    "#### Hiperparametros utilizados no exemplo:\n",
    "*make_classification:* Função da scikit-learn para criar um conjunto de dados sintético para classificação.\n",
    "\n",
    "* n_samples=1000: Cria um total de 1000 amostras no conjunto de dados.\n",
    "\n",
    "* n_features=4: Cada amostra terá 4 características (ou features).\n",
    "\n",
    "* n_informative=2: Apenas 2 das 4 características são informativas para a tarefa de classificação. Ou seja, essas 2 características realmente ajudam a separar as classes.\n",
    "\n",
    "* n_redundant=0: Não há características redundantes. Características redundantes são combinações lineares das características informativas.\n",
    "\n",
    "* random_state=0: Define uma semente para o gerador de números aleatórios, garantindo que o conjunto de dados gerado seja o mesmo a cada execução.\n",
    "\n",
    "* shuffle=False: Não embaralha as amostras. As amostras são geradas e retornadas na ordem em que foram criadas.\n",
    "\n",
    "* X: Matriz de características (1000 amostras x 4 características)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c46cdf",
   "metadata": {},
   "source": [
    "### GridSearch para encontrar os melhores hiperparametros para o conjunto Iris:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce3f1cc",
   "metadata": {},
   "source": [
    "Agora vamos combinar alguns hiperparametros e descobrir através do GridSearch quais são os melhores. Para começar vamos combinar o n_estimators e o learning_rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a0e0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'learning_rate': 0.1, 'n_estimators': 150, 'random_state': 0}\n",
      "Melhor acurácia encontrada:\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# Carregar o conjunto de dados\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Configurar o classificador AdaBoost\n",
    "clf = AdaBoostClassifier(algorithm=\"SAMME\")\n",
    "\n",
    "# Definir a grade de parâmetros para o GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "    'random_state': [0, 10, 20]\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Ajustar o GridSearchCV aos dados\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Obter a melhor pontuação (acurácia) encontrada\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Melhores parâmetros encontrados:\")\n",
    "print(best_params)\n",
    "print(\"Melhor acurácia encontrada:\")\n",
    "print(best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac68877",
   "metadata": {},
   "source": [
    "Obtemos uma boa acurácia. Vale ressaltar que com essas listas para esses três parametros conseguimos o mesmo resultado que no exemplo acima aonde temos um número maior de parametros sendo utilizados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070250a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
