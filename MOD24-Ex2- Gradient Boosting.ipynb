{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095d39d4",
   "metadata": {},
   "source": [
    "### Cinco diferenças entre AdaBoost e Gradient Boosting:\n",
    "\n",
    "#### Florestas:\n",
    "\n",
    "* AdaBoost:floresta de strumps (um nó e duas folhas) e cada strump influenciará na criação do strump posterior (bootstrap: amostragem com reposição). Cada strump terá um peso diferente na hora da resposta, a dependender do seu potencial preditivo. \n",
    "* Gradient Boosting: Floresta de árvores. Todas as respostas das árvores possuem um multiplicador em comum chamado learning_rate(eta).\n",
    "\n",
    "#### Treinamento de Modelos:\n",
    "\n",
    "* AdaBoost: Treina modelos sequencialmente, ajustando o foco para exemplos que foram mal classificados pelos modelos anteriores. O treinamento é adaptativo e os modelos são ajustados com base nos erros dos modelos anteriores.\n",
    "* Gradient Boosting: O processo de Gradient Boosting ajusta iterativamente um modelo base para melhorar as previsões. Inicialmente, um modelo simples (média) faz a previsão. Em cada iteração seguinte, um novo modelo é treinado para prever os resíduos do modelo atual, e essas previsões são adicionadas ao modelo existente, ponderadas pela taxa de aprendizado. Esse processo é repetido para um número definido de iterações ou até que o modelo alcance um desempenho satisfatório.\n",
    "\n",
    "#### Peso das Respostas das Árvores:\n",
    "\n",
    "* AdaBoost: O modelo final é uma média ponderada das árvores, que leva em consideração o potencial preditivo de cada strump.\n",
    "* Gradient Boosting: O modelo final é uma média das árvores, sendo que cada uma delas possui um multiplicador em comum que se chama learning_rate (eta), esse multiplicador geralmente recebe valores até 0.1 pois uma taxa de aprendizado mais baixa exige mais iterações e pode resultar em um modelo mais robusto e menos suscetível ao overfitting, mas há casos que essa taxa pode ser maior para melhorar a velocidade do modelo, podendo variar de 0 a 1. \n",
    "\n",
    "\n",
    "#### Desempenho e Ajuste:\n",
    "* Adaboost: Geralmente requer menos ajuste e é mais fácil de implementar, mas pode ter desempenho inferior em conjuntos de dados com alto ruído. A redução de viés é a principal meta.\n",
    "* Gradient Boosting: Oferece mais controle sobre o modelo através de vários hiperparâmetros (como taxa de aprendizado, profundidade das árvores e número de árvores), permitindo ajustes finos para evitar overfitting. Pode ser mais complexo de ajustar e implementar, mas oferece maior flexibilidade e potencial de desempenho.\n",
    "\n",
    "\n",
    "#### Estratégia de Regularização:\n",
    "* Adaboost: A regularização é implícita e ocorre através do ajuste dos pesos das instâncias de treinamento. Ao dar mais peso às instâncias mal classificadas, o AdaBoost pode ser mais suscetível ao overfitting em conjuntos de dados ruidosos. No entanto, a simplicidade dos modelos fracos (stumps) ajuda a limitar a complexidade e, portanto, a regularização é parcialmente gerenciada pela escolha dos modelos e pela forma como os erros são ponderados.\n",
    "* Gradient Boosting: Oferece várias técnicas explícitas de regularização para controlar o ajuste do modelo e prevenir overfitting, como a profundidade das árvores que não pode ser controlada no AdaBoost já que se trata de stumps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bea028",
   "metadata": {},
   "source": [
    "### Exemplo de Classificação e Regressão do GBM:\n",
    "Os exemplos a seguir foram retirados [(aqui)](https://scikit-learn.org/stable/modules/ensemble.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff258c2",
   "metadata": {},
   "source": [
    "#### Classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80677abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0) \n",
    "\n",
    "#X_train e y_train: Contêm os primeiros 2000 exemplos do conjunto. X_test e y_test: Contêm o restante. \n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc7ce1",
   "metadata": {},
   "source": [
    "#### Explicação do Código:\n",
    "\n",
    "* make_hastie_10_2: É uma função que gera um conjunto de dados sintético com duas classes. Este conjunto de dados é criado para simular um problema de classificação binária com características baseadas em um modelo de mistura gaussiana. O random state garante a reprodutibilidade desses dados para testes futuros.\n",
    "\n",
    "* train e test: treino recebe os primeiros dois mil exemplos e teste o restante.\n",
    "\n",
    "* GradientBoostingClassifier: É um classificador que utiliza o algoritmo de boosting, neste caso está utilizando alguns dos parâmetros que serão explicados abaixo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb900ba7",
   "metadata": {},
   "source": [
    "#### Regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65702c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0) #gera conjunto com valores contínuos com 1200 amostras e ruído para tornar o problema mais realista.\n",
    "\n",
    "#train: primeiros 200, test: restante\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error' #função de perda a ser minimizada durante o treinamento.\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ca55d",
   "metadata": {},
   "source": [
    "#### Explicação do Código:\n",
    "\n",
    "* X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0):  gera conjunto com valores contínuos com 1200 amostras, random state para garantir reprodutibilidade e ruído para tornar o problema mais realista.\n",
    "* train: primeiros 200 exemplos, test: restante.\n",
    "* GradientBoostingRegressor: modelo de regressão com alguns dos parâmetros explicados na próxima célula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9fd70",
   "metadata": {},
   "source": [
    "### Hiperparametros importantes no GBM:\n",
    "* max_features: Número máximo de características a serem consideradas para a melhor divisão em cada nó. Reduzir esse número pode aumentar a diversidade entre as árvores e melhorar a generalização.\n",
    "\n",
    "* min_samples_leaf: Número mínimo de amostras necessárias para estar em um nó folha. Pode ajudar a controlar a complexidade das árvores.\n",
    "\n",
    "* loss: Função de perda a ser minimizada. Pode ser 'squared_error', 'absolute_error', 'poisson', entre outros, dependendo do tipo de problema (regressão ou classificação).\n",
    "\n",
    "* n_estimators: Número de árvores (ou estimadores) a serem usadas no passo a passo.  Um número maior de estimadores geralmente melhora a performance do modelo, pois mais árvores podem capturar padrões mais complexos. No entanto, pode também aumentar o risco de overfitting e o tempo de treinamento. Ajustar esse parâmetro envolve pensar um custo-benefício entre desempenho e tempo de computação.\n",
    "\n",
    "* learning_rate: Taxa de aprendizado, que controla o impacto de cada árvore na previsão final. Uma taxa de aprendizado menor faz com que o modelo aprenda mais lentamente, o que geralmente requer mais árvores (n_estimators) para alcançar um desempenho ótimo. Taxas de aprendizado mais altas podem acelerar o treinamento, mas podem causar overfitting ou levar a um modelo menos robusto.\n",
    "\n",
    "* max_depth: Profundidade máxima das árvores de decisão individuais. Árvores mais profundas podem capturar padrões mais complexos nos dados, mas também têm um risco maior de overfitting. Árvores rasas (menor profundidade) são mais simples e tendem a generalizar melhor, mas podem não capturar toda a complexidade dos dados.\n",
    "\n",
    "* min_samples_split: Número mínimo de amostras necessárias para dividir um nó interno na árvore. Aumentar esse parâmetro faz com que as árvores se tornem mais conservadoras, reduzindo o risco de overfitting, mas pode levar a árvores muito simples e menos eficazes. Um valor menor pode resultar em árvores mais complexas, mas com risco maior de overfitting.\n",
    "\n",
    "* subsample: Fração das amostras usadas para treinar cada árvore. Também conhecido como \"bagging\". Usar uma fração menor das amostras para treinar cada árvore pode ajudar a reduzir o overfitting e aumentar a robustez do modelo. No entanto, se o valor for muito baixo, o modelo pode não ter informações suficientes para aprender padrões complexos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83e573",
   "metadata": {},
   "source": [
    "### Encontrando os melhores hiperparametros utilizando GridSearch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b559d1b",
   "metadata": {},
   "source": [
    "#### Exemplo de Classificação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119a60b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Acurácia no conjunto de teste: 0.91\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Gerar o conjunto de dados\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Definir a grade de parâmetros para GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=0),\n",
    "    param_grid,\n",
    "    cv=5,  # Validação cruzada com 5 folds\n",
    "    scoring='accuracy',  # Usar acurácia como métrica de avaliação\n",
    "    n_jobs=-1  # Usar todos os núcleos disponíveis\n",
    ")\n",
    "\n",
    "# Ajustar o GridSearchCV aos dados de treinamento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obter os melhores parâmetros e o melhor modelo\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Avaliar o desempenho do melhor modelo no conjunto de teste\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Melhores parâmetros encontrados:\", best_params)\n",
    "print(\"Acurácia no conjunto de teste:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dfc815",
   "metadata": {},
   "source": [
    "O resultado obtido foi muito semelhante ao resultado trazido pelo exemplo da documentação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46040d",
   "metadata": {},
   "source": [
    "### Exemplo de Regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da222b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Erro Quadrático Médio (MSE) no conjunto de teste: 3.5218851745037365\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Gerar o conjunto de dados\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "# Definir a grade de parâmetros para GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=0),\n",
    "    param_grid,\n",
    "    cv=5,  # Validação cruzada com 5 folds\n",
    "    scoring='neg_mean_squared_error',  # Usar MSE negativo para maximizar\n",
    "    n_jobs=-1  # Usar todos os núcleos disponíveis\n",
    ")\n",
    "\n",
    "# Ajustar o GridSearchCV aos dados de treinamento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obter os melhores parâmetros e o melhor modelo\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Avaliar o desempenho do melhor modelo no conjunto de teste\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "print(\"Melhores parâmetros encontrados:\", best_params)\n",
    "print(\"Erro Quadrático Médio (MSE) no conjunto de teste:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa984f5",
   "metadata": {},
   "source": [
    "O resultado obtido foi melhor que o resultado trazido pela documentação, primeiro por testar mais parâmetros e pelas combinações que fez com esses parâmetros. O que 'peca' aqui é o tempo de processamento. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2e134",
   "metadata": {},
   "source": [
    "### Principal diferença entre AdaBoost e Stochastic Gradient Boosting:\n",
    "\n",
    " A principal diferença entre AdaBoost e Stochastic Gradient Boosting (GBM) está na abordagem de amostragem durante o treinamento dos modelos. O AdaBoost ajusta o modelo base utilizando todas as amostras disponíveis e altera os pesos das amostras com base nos erros das iterações anteriores, realizando uma forma de amostragem com reposição, onde todas as amostras são utilizadas em cada iteração, mas com pesos ajustados. Em contraste, o Stochastic GBM adota uma abordagem de amostragem sem reposição, selecionando aleatoriamente uma fração dos dados para treinar o modelo em cada iteração. Essa amostragem estocástica ajuda a criar um modelo mais robusto e reduz o risco de overfitting, além de acelerar o treinamento ao diminuir a quantidade de dados utilizados em cada iteração, ao contrário da abordagem mais intensiva do AdaBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
